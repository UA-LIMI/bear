# Product Requirements Document (PRD): Limi AI - Real-time Voice Assistant

## 1. Overview

Limi AI is a modern, real-time voice AI application featuring a sophisticated frontend built with Next.js and a secure, production-ready Node.js/Express backend. The core of the application is its ability to establish a direct, real-time voice connection with OpenAI's advanced AI models. The backend serves as a secure gateway, managing API keys and providing short-lived, ephemeral tokens to the frontend, which then communicates directly with the OpenAI Realtime API. This architecture ensures both high performance and robust security, preventing any exposure of sensitive API keys to the client-side.

The project is managed using the `task-master-ai` CLI, a powerful tool for orchestrating the development workflow, from task generation and dependency management to progress tracking.

## 2. Core Features

### 2.1. Frontend: Real-time Voice Interface
- **Next.js & TypeScript:** A modern, type-safe, and performant frontend application.
- **`VoiceConnection.tsx` Component:** A sophisticated, animated UI component for managing the voice connection lifecycle.
- **Connection States:** The UI will visually represent all connection states: `disconnected`, `connecting`, `connected`, `listening`, `processing`, `speaking`, and `failed`.
- **Configuration Panel:** Users can configure the AI model, voice style, and provide custom instructions to the AI.
- **Real-time Audio Visualization:** The UI will feature real-time audio visualizations to provide feedback to the user.
- **Error Handling:** Clear, user-friendly error messages will be displayed on the UI.

### 2.2. Backend: Secure AI Gateway
- **Node.js & Express:** A robust, production-ready backend built with a standard, well-supported technology stack.
- **Ephemeral Token Generation:** The backend's primary responsibility is to generate secure, short-lived tokens for the frontend to use when connecting to OpenAI.
- **API Key Management:** All sensitive API keys are stored securely on the backend and never exposed to the client.
- **Production-Ready Architecture:**
    - **Cloud-Native Health Checks:** Includes `/healthz`, `/readyz`, `/live`, and `/status` endpoints for comprehensive monitoring.
    - **Robust Security:** Implements Helmet for security headers, CORS, rate limiting, and comprehensive input validation.
    - **Structured Logging:** Uses Winston for structured, centralized logging with request tracing.
    - **Centralized Configuration:** All configuration is managed via a central module with support for environment variables.

## 3. Technical Architecture

### 3.1. Frontend
- **Framework:** Next.js 15+ with TypeScript
- **UI:** Shadcn UI components, Framer Motion for animations, Tailwind CSS
- **Real-time Communication:** `@openai/agents-realtime` for direct communication with OpenAI.

### 3.2. Backend
- **Framework:** Node.js 18+ with Express
- **Key Dependencies:** `dotenv`, `cors`, `helmet`, `express-rate-limit`, `express-validator`, `winston`.
- **Security:** Implements a standard middleware chain for security, logging, and validation.

### 3.3. Workflow
1.  The frontend requests an ephemeral token from the backend's `/api/client-secret` endpoint.
2.  The backend validates the request and generates a short-lived token using its secure OpenAI API key.
3.  The frontend receives the ephemeral token.
4.  The frontend uses the ephemeral token to establish a direct, real-time connection to the OpenAI API.
5.  All subsequent voice communication happens directly between the frontend and OpenAI, ensuring low latency.

## 4. Development Roadmap (Generated Tasks)

### Phase 1: Foundational Setup & Backend Solidification
- **Task 1:** Finalize Backend Infrastructure (complete pending tasks from `REVIEW.md`).
    - **Subtask 1.1:** Implement Centralized Error Handling.
    - **Subtask 1.2:** Enhance Logging with file logging and rotation.
    - **Subtask 1.3:** Create Comprehensive API Documentation.
- **Task 2:** Frontend & Backend Integration.
    - **Subtask 2.1:** Ensure the frontend can successfully connect to the running backend and retrieve an ephemeral token.
    - **Subtask 2.2:** Test the full, end-to-end connection flow to OpenAI.
- **Task 3:** Vercel Deployment & CI/CD.
    - **Subtask 3.1:** Resolve any remaining Vercel deployment issues.
    - **Subtask 3.2:** Configure Vercel environment variables for the backend.

### Phase 2: Feature Enhancement & Production Hardening
- **Task 4:** Frontend UI/UX Polish.
    - **Subtask 4.1:** Refine UI animations and state transitions.
    - **Subtask 4.2:** Conduct user experience testing and gather feedback.
- **Task 5:** Advanced Backend Features.
    - **Subtask 5.1:** Implement Application Performance Monitoring (APM).
    - **Subtask 5.2:** Conduct a formal security audit.
    - **Subtask 5.3:** Conduct performance and load testing.

## 5. Risks and Mitigations

- **Risk:** Latency in real-time communication.
  - **Mitigation:** The current architecture, with a direct frontend-to-OpenAI connection, is designed to minimize this risk. Performance testing will be critical.
- **Risk:** Security vulnerabilities.
  - **Mitigation:** The backend is built with a security-first approach. A formal security audit will be conducted to further mitigate this risk.
- **Risk:** Scalability.
  - **Mitigation:** The backend's cloud-native design and the direct communication pattern will support scalability. Load testing will be used to identify and address any bottlenecks.
