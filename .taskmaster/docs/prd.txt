# Product Requirements Document (PRD): LIMI AI - Hotel Guest Experience Platform

## 1. Executive Summary

LIMI AI is a voice-first hotel guest experience platform that provides intelligent, contextual assistance through adaptive user interfaces. The system dynamically adjusts its functionality and AI personality based on guest status, location, and preferences, delivering personalized service for hotel operations, city exploration, and travel planning.

**Current Status:** Production-ready voice AI infrastructure with hotel guest experience features in active development.

Built on proven WebRTC voice connection capabilities, the system extends core AI functionality with hotel-specific features, user management, and contextual intelligence that adapts to guest needs.

## 2. Core Features

### 2.1. Frontend: Real-time Voice Interface
- **Next.js & TypeScript:** A modern, type-safe, and performant frontend application.
- **`VoiceConnection.tsx` Component:** A sophisticated, animated UI component for managing the voice connection lifecycle.
- **Connection States:** The UI will visually represent all connection states: `disconnected`, `connecting`, `connected`, `listening`, `processing`, `speaking`, and `failed`.
- **Configuration Panel:** Users can configure the AI model, voice style, and provide custom instructions to the AI.
- **Real-time Audio Visualization:** The UI will feature real-time audio visualizations to provide feedback to the user.
- **Error Handling:** Clear, user-friendly error messages will be displayed on the UI.

### 2.2. Backend: Secure AI Gateway
- **Node.js & Express:** A robust, production-ready backend built with a standard, well-supported technology stack.
- **Ephemeral Token Generation:** The backend's primary responsibility is to generate secure, short-lived tokens for the frontend to use when connecting to OpenAI.
- **API Key Management:** All sensitive API keys are stored securely on the backend and never exposed to the client.
- **Production-Ready Architecture:**
    - **Cloud-Native Health Checks:** Includes `/healthz`, `/readyz`, `/live`, and `/status` endpoints for comprehensive monitoring.
    - **Robust Security:** Implements Helmet for security headers, CORS, rate limiting, and comprehensive input validation.
    - **Structured Logging:** Uses Winston for structured, centralized logging with request tracing.
    - **Centralized Configuration:** All configuration is managed via a central module with support for environment variables.

## 3. Technical Architecture

### 3.1. Frontend
- **Framework:** Next.js 15+ with TypeScript
- **UI:** Shadcn UI components, Framer Motion for animations, Tailwind CSS
- **Real-time Communication:** `@openai/agents-realtime` for direct communication with OpenAI.

### 3.2. Backend
- **Framework:** Node.js 18+ with Express
- **Key Dependencies:** `dotenv`, `cors`, `helmet`, `express-rate-limit`, `express-validator`, `winston`.
- **Security:** Implements a standard middleware chain for security, logging, and validation.

### 3.3. Workflow
1.  The frontend requests an ephemeral token from the backend's `/api/client-secret` endpoint.
2.  The backend validates the request and generates a short-lived token using its secure OpenAI API key.
3.  The frontend receives the ephemeral token.
4.  The frontend uses the ephemeral token to establish a direct, real-time connection to the OpenAI API.
5.  All subsequent voice communication happens directly between the frontend and OpenAI, ensuring low latency.

## 4. Development Roadmap (Generated Tasks)

### Phase 1: Foundational Setup & Backend Solidification
- **Task 1:** Finalize Backend Infrastructure (complete pending tasks from `REVIEW.md`).
    - **Subtask 1.1:** Implement Centralized Error Handling.
    - **Subtask 1.2:** Enhance Logging with file logging and rotation.
    - **Subtask 1.3:** Create Comprehensive API Documentation.
- **Task 2:** Frontend & Backend Integration.
    - **Subtask 2.1:** Ensure the frontend can successfully connect to the running backend and retrieve an ephemeral token.
    - **Subtask 2.2:** Test the full, end-to-end connection flow to OpenAI.
- **Task 3:** Vercel Deployment & CI/CD.
    - **Subtask 3.1:** Resolve any remaining Vercel deployment issues.
    - **Subtask 3.2:** Configure Vercel environment variables for the backend.

### Phase 2: Feature Enhancement & Production Hardening
- **Task 4:** Frontend UI/UX Polish.
    - **Subtask 4.1:** Refine UI animations and state transitions.
    - **Subtask 4.2:** Conduct user experience testing and gather feedback.
- **Task 5:** Advanced Backend Features.
    - **Subtask 5.1:** Implement Application Performance Monitoring (APM).
    - **Subtask 5.2:** Conduct a formal security audit.
    - **Subtask 5.3:** Conduct performance and load testing.

## 5. Risks and Mitigations

- **Risk:** Latency in real-time communication.
  - **Mitigation:** The current architecture, with a direct frontend-to-OpenAI connection, is designed to minimize this risk. Performance testing will be critical.
- **Risk:** Security vulnerabilities.
  - **Mitigation:** The backend is built with a security-first approach. A formal security audit will be conducted to further mitigate this risk.
- **Risk:** Scalability.
  - **Mitigation:** The backend's cloud-native design and the direct communication pattern will support scalability. Load testing will be used to identify and address any bottlenecks.
